expgram: EXPonential-order n-GRAM toolkit

This is an ngram package with efficient handling of large data in mind, based on a succinct storage.
The target is to index Google's ngrams into 10GB.
Also, this toolkit tries to implement large scale ngram language model proposed by Goolge[1]

Brief descriptions:

      ngram_counts_extract
	compute ngram counts given  sentence data or ngram counts collection. The output is almost compatible with
	Google ngram data specification, but differ in that counts are not reduced.

      ngram_counts_index
	ngram counts indexer from Googles ngram data format.
	--shard control # of threads, which implies # of shards (or # of data splitting)

      ngram_counts_modify
	perform counts modification in ngram data for KN smoothing. If not indexed, perform indexing.

      ngram_counts_estimate
	perform ngram probabilities estimation. If not indexed, perfomr indexing. If not modified, perform counts modification.

      ngram_index
	ngram indexer from arpa format

      ngram_bound
	ngram upper bound estimator. Compute upper bounds for ngrams. You do not have to run this, since ngram_index will
	compute upper bounds simultaneously. It remains here for compatibility with mpi version (see below).

      ngram_quantize
	perform quantization for ngram probabilities, backoffs and upper bounds. If not indexed, perform indexing.

      ngram_stat/ngram_counts_stat
	dump statistics on storage size

     The set of tools provided here extensively use temporary disk space specified either by TMPDIR or TMPDIR_SPEC.
     
     TMPDIR is usually set by default, and usually specified /tmp or /var/tmp

     Alternatively, you can specify via TMPDIR_SPEC: 
     
     export TMPDIR_SPEC=/export/%host/users/${USER}
     
     THe %host key word is simply replaced by the host of running machine (so that each binary may abuse only local-storage).
          	 

For larger data, it is recommended to use mpi-version for scalability. (Currently, experimental, and NOT TESTED.)

      ngram_counts_extract_mpi
      ngram_counts_index_mpi
      ngram_counts_modify_mpi
      ngram_counts_estimate_mpi
      ngram_bound_mpi
      ngram_quantize_mpi

They performed similar to threaded version, but differ in that you have to explicitly run from index though quantize in order.

API: Sample codes exists at sample directory, ngram.cc and ngram_counts.cc

	NGram:
		operator()(first, last) : return backoff log-probabilities for ngram. Iterator must supports random-access concepts, such as
			     	  	vector's iterator.

		logprob(first, last) : synonym to operator()(first, last)
		
		logbound(first, last) : Return upper-bound log-probability for ngram. Specifically, P_{bound}(w_n | w_i ... w_{n-1}) = max_{w_{i-1}} P(w_n | w_{i-1}, ... w_{n-1})
				      	It is useful for a task, such as decoding, when we want to pre-compute heuristic score in advance.
		
		exists(first, last) : check whether a particular ngram exists or not.

		index.order(): returns ngram's maximum order

	NGramCounts:
		operator()(first, last) : return ngram count. if an ngram [first, last) does not exist, return zero.

		count(first, last) : synonym to operator()(first, last)
		
		exists(first, last) : check whether a particula ngram exists or not.
			    
		index.order(): returns ngram's maximum order
	
	Internally, words (assuming std::string) are autoamtically converted into word_type (expgram::Word), then word_id
	(expgram::Word::id_type). If you want to avoid such conversion on-the-fly, you can pre convert them by
		
		expgram::Word::id_type word_id = {ngram or ngram_counts}.index.vocab()[word];
	
	All the operations are thread-safe, meaning that concurrent programs may call any API any time without locking!
	(except for ngram loading...)


Requirements:
	Following table lists required libraries for installation. The tools are successfully compiled
	with GCC 4.0.0 (Mac OS X). However, it is recommended to use the latest stable GCC 4.3.3.

	Libraries			version		URL
	--------------------------------------------------------------------------
	Boost Library			1.38.0		http://www.boost.org
	MPI (Open MPI)			1.3.1		http://www.open-mpi.org
	google-perftools (tcmalloc)	1.1		http://code.google.com/p/google-perftools/
	google-sparsehash		1.4		http://code.google.com/p/google-sparsehash/
	ICU				4.2.0		http://site.icu-project.org/
	hunspell			1.2.8		http://hunspell.sourceforge.net/

Imported Libraries:
	There exists a library imported in this ditribution.

        Libraries                       version         URL
        --------------------------------------------------------------------------
	Snowball			???		http://snowball.tartarus.org/

References:
[1]
@InProceedings{brants-EtAl:2007:EMNLP-CoNLL2007,
  author    = {Brants, Thorsten  and  Popat, Ashok C.  and  Xu, Peng  and  Och, Franz J.  and  Dean, Jeffrey},
  title     = {Large Language Models in Machine Translation},
  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},
  month     = {June},
  year      = {2007},
  address   = {Prague, Czech Republic},
  publisher = {Association for Computational Linguistics},
  pages     = {858--867},
  url       = {http://www.aclweb.org/anthology/D/D07/D07-1090}
}

