expgram: EXPonential-order n-GRAM toolkit

This is an ngram package with efficient handling of large data in mind, based on a succinct storage [1].
The target is to index Google's ngrams into 10GB.
Among ngram compression mentioned in [1], we do not implement block-wise compression (or gzip every 8k-byte)
for efficiency reason. 
Also, this toolkit implements large scale ngram language model proposed by Goolge [2].

Brief descriptions:

      ngram_vocab
        Compute vocabulary (unigram with count). From this file, you can create count thresholded word-list, for instance, by
	 
        cat [output of ngram_vocab] | gawk '{ if ($12 >= 20) {print $1;}}' > vocab.list

      ngram_counts_extract
	compute ngram counts given  sentence data or ngram counts collection. The output is almost compatible with
	Google ngram data specification, but differ in that counts are not reduced.
	If you want to restrict vocabulary, supply word-list via --vocab option.

      ngram_counts_index
	ngram counts indexer from Googles ngram data format.
	--shard control # of threads, which implies # of shards (or # of data splitting)

      ngram_counts_modify
	perform counts modification in ngram data for KN smoothing. If not indexed, perform indexing.

      ngram_counts_estimate
	perform ngram probabilities estimation. If not indexed, perfomr indexing. If not modified, perform counts modification.

      ngram_index
	ngram indexer from arpa format

      ngram_bound
	ngram upper bound estimator. Compute upper bounds for ngrams. You do not have to run this, since ngram_index will
	compute upper bounds simultaneously. It remains here for compatibility with mpi version (see below).

      ngram_quantize
	perform quantization for ngram probabilities, backoffs and upper bounds. If not indexed, perform indexing.

      ngram_stat/ngram_counts_stat
	dump statistics on storage size

      expgram_clearn
	If you found spurious data on your temporary directory, run this to clean up the data. 
	REMARK: make sure you are not runnning other expgram tools!

     The set of tools provided here extensively use temporary disk space specified either by TMPDIR or TMPDIR_SPEC.
     TMPDIR is usually set by default, and usually specified /tmp or /var/tmp
     Alternatively, you can specify via TMPDIR_SPEC: 
     
     export TMPDIR_SPEC=/export/%host/users/${USER}
     
     The %host key word is simply replaced by the host of running machine (so that each binary may abuse only local-storage).
     RECENT UPDATE: Under NICT environment, you can try:

     export TMPDIR_SPEC=/export/tmp


For larger data, it is recommended to use mpi-version for scalability.

      ngram_vocab_mpi
      ngram_counts_extract_mpi
      ngram_counts_index_mpi
      ngram_counts_modify_mpi
      ngram_counts_estimate_mpi
      ngram_bound_mpi
      ngram_quantize_mpi

They performed similar to threaded version, but differ in that you have to explicitly run from index though quantize in order.

API: Sample codes exists at sample directory, ngram.cc and ngram_counts.cc

	NGram:
		operator()(first, last) : return backoff log-probabilities for ngram. Iterator must supports random-access
				  	  concepts, such as vector's iterator.

		logprob(first, last) : synonym to operator()(first, last)
		
		logbound(first, last) : Return upper-bound log-probability for ngram. Specifically,
				        P_{bound}(w_n | w_i ... w_{n-1}) = max_{w_{i-1}} P(w_n | w_{i-1}, ... w_{n-1}).
				      	It is useful for a task, such as decoding, when we want to pre-compute heuristic
					score in advance.
		
		exists(first, last) : check whether a particular ngram exists or not.

		index.order(): returns ngram's maximum order
		
	NGramCounts:
		operator()(first, last) : return ngram count. if an ngram [first, last) does not exist, return zero.
		
		count(first, last) : synonym to operator()(first, last)
		
		exists(first, last) : check whether a particular ngram exists or not.
		
		index.order(): returns ngram's maximum order
	
	Internally, words (assuming std::string) are autoamtically converted into word_type (expgram::Word), then word_id
	(expgram::Word::id_type). If you want to avoid such conversion on-the-fly, you can pre convert them by
		
		expgram::Word::id_type word_id = {ngram or ngram_counts}.index.vocab()[word];

	or, if you don't want to waste extra memory for expgram::Word type, use: (assuming "word" is std::string)

                expgram::Word::id_type word_id = {ngram or ngram_counts}.index.vocab()[expgram::Vocab::UNK];
		if ({ngram or ngram_counts}.index.vocab().exists(word))
		  word_id = {ngram or ngram_counts}.index.vocab()[word];

	
	All the operations are thread-safe, meaning that concurrent programs may call any API any time without locking!
	(except for ngram loading...)

Requirements:
	Following table lists required libraries for installation. The tools are successfully compiled
	with GCC 4.0.0 (Mac OS X). However, it is recommended to use the latest stable GCC 4.3.3.

	Libraries			version		URL
	--------------------------------------------------------------------------
	Boost Library			1.38.0		http://www.boost.org
	MPI (Open MPI)			1.3.1		http://www.open-mpi.org
	google-perftools (tcmalloc)	1.1		http://code.google.com/p/google-perftools/
	google-sparsehash		1.4		http://code.google.com/p/google-sparsehash/

References:
[1]
@InProceedings{watanabe-tsukada-isozaki:2009:Short,
  author    = {Watanabe, Taro  and  Tsukada, Hajime  and  Isozaki, Hideki},
  title     = {A Succinct N-gram Language Model},
  booktitle = {Proceedings of the ACL-IJCNLP 2009 Conference Short Papers},
  month     = {August},
  year      = {2009},
  address   = {Suntec, Singapore},
  publisher = {Association for Computational Linguistics},
  pages     = {341--344},
  url       = {http://www.aclweb.org/anthology/P/P09/P09-2086}
}

[2]
@InProceedings{brants-EtAl:2007:EMNLP-CoNLL2007,
  author    = {Brants, Thorsten  and  Popat, Ashok C.  and  Xu, Peng  and  Och, Franz J.  and  Dean, Jeffrey},
  title     = {Large Language Models in Machine Translation},
  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},
  month     = {June},
  year      = {2007},
  address   = {Prague, Czech Republic},
  publisher = {Association for Computational Linguistics},
  pages     = {858--867},
  url       = {http://www.aclweb.org/anthology/D/D07/D07-1090}
}

